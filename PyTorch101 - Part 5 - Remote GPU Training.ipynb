{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch101 - Part 5 - Remote GPU Training\n",
    "In this part of the tutorial, we will discover how to train on GPUs while also understanding how to train remotely and saving a model, to make inference offline afterwards.\n",
    "\n",
    "We decided not to use a remote jupyter notebook just replicated as before; instead, we decided to create an end-to-end training script which is launched directly on the remote machine. This script includes the data loading and the tensorboard logging as before.\n",
    "\n",
    "This notebook is mostly intended as a presentation for new instructions, namely the GPU execution and the model saving/loading, but also as a presentation of the results, both in terms of accuracy of the local inference and on time requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n",
      "0.2.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Training: what we need to do\n",
    "To effectively use a GPU for computation, we need to move everything (model, parameters and data) to the GPU device. In PyTorch, each tensor resides in a specific device: this is quite different from tensorflow, where it simply uses GPU when it can and it does not provide any manual control on where data resides.\n",
    "\n",
    "To move tensors to GPU it suffice to call the *.to(device)* instruction, where device can be both a CPU or a GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the available device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move network to device\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model\n",
    "The procedure to save a model depends on wether we want to continue training o just use the local copy for inference. Basically, if we want to continue training, we also need to save the current epoch and the optimizer state. When we use the model for inference, we call the *.eval()* method\n",
    "\n",
    "### Model used for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "torch.save(model.state_dict(), filepath)\n",
    "\n",
    "# Load the model for inference\n",
    "model.load_state_dict(torch.load(filepath))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cumulative state\n",
    "state = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    # Can also include scores and other\n",
    "}\n",
    "# Save \n",
    "torch.save(state, filepath)\n",
    "\n",
    "# Loading to continue training\n",
    "model.load_state_dict(state['state_dict'])\n",
    "optimizer.load_state_dict(stata['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
